################################################################
# this image meets all requirements resources to start jmeter
FROM alpine as jmeter_dependencies

# install all required dependencies
RUN apk add git bash openjdk8 python3 py3-pip maven
RUN python3 -m pip install --upgrade pip && pip install prettytable requests

# base environments
ENV WDIR /jmeter
ENV BENCHMARK_DIR ${WDIR}/benchmarks

# set working directory in container
WORKDIR ${WDIR}
# get required resources from current build dir
ADD . ${BENCHMARK_DIR}

# configure java
ENV JAVA_ARGS -Xms4g -Xmx4g

# configure jmeter
RUN bash ${BENCHMARK_DIR}/setup_jmeter.bash
ENV JMETER="${WDIR}/jmeter/apache/bin/jmeter"

# configure file/dir server
RUN git clone https://github.com/simon-budig/woof.git
ENV WOOF "${WDIR}/woof/woof"

# configure M2U
RUN bash ${BENCHMARK_DIR}/setup_m2u.bash
ENV M2U="java -jar ${WDIR}/m2u/target/m2u.jar"

################################################################
# this image contains set of rules to start benchmark tests
FROM jmeter_dependencies AS benchmark_aio

WORKDIR ${BENCHMARK_DIR}

# api to test
ENV API="account_history_api"

# input file to use for performance testing
ENV CSV="perf_60M_heavy.csv"

# amount of threads
ENV JOBS=10

# amount of requests per thread (-1 for infinite)
ENV LOOPS=500

# possible options: old-style, new-style, postgres
ENV CALL_STYLE="old-style"

# address to test (default is set to default host address in docker)
ENV ADDRESS='172.17.0.1'

# port to perform tests
ENV PORT=8090

# url to postgres database (required only if CALL_STYLE = postgres)
ENV POSTGRES_URL="postgresql:///haf_block_log"

# schema in which functions to test are (required only if CALL_STYLE = postgres)
ENV POSTGRES_SCHEMA="hive"

# path to root directory of tests_api project (can be set on CI to /build/path/to/hive/tests/tests_api)
ENV ROOT_DIR="${WDIR}"

# if set, start hosting workdir after benchmarking on specified port (just one time), remember to expose that port
ENV SERVE_PORT=""

# additional arguments that will be passed to benchmarking script
ENV ADDITIONAL_ARGS=""

# path to directory, where jmeter and python benchmark script will put all it's output
ENV JMETER_WORKDIR=${ROOT_DIR}/wdir

# verification is setup ready
RUN python3 benchmark.py -h
RUN python3 benchmark.py -n ${API} -l

# defines what to do after docker starts
ENTRYPOINT bash ${ROOT_DIR}/benchmarks/docker/entrypoint.bash $ADDITIONAL_ARGS
